---
title: "Random Forest and Logistic Regression on Colinear Data"
author: "Emma Collins"
date: "Winter 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE}
library(boot)         # for cv.glm()
library(faraway)      # for vif()
library(StepReg)      # for stepwise model selection
library(randomForest) # for random forest
library(ggplot2)      # for ROC plots
library(plotROC)      # fot ROC plots
library(RColorBrewer) # pretty colors for plots

set.seed(22)          # reproducability

### Pretty color prep
display.brewer.all(type = "all", colorblindFriendly = TRUE)
mycol <- brewer.pal(2, "Paired")
```

The goal of this project is to understand how well two different classification methods perform on highly collinear data.  The Sonar dataset is used, provided by the University of California, Irvine's Machine Learning Repository. 
  
## Data Understanding 

### Summary Statistics

```{r data}
### Link to dataset
# https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)

sonar <- read.delim("C:\\Users\\Owner\\Desktop\\Portfolio\\RFvLog\\sonar.all-data", header = FALSE, sep = ",")
head(sonar)

#summary(sonar)

#rename response column for easier identification
names(sonar)[names(sonar) == "V61"] <- "Y"

# inital look at data for rocks and mines
rocks <- subset(sonar, sonar$Y == "R")
mines <- subset(sonar, sonar$Y == "M")

```

We will use inital plots to infer if there are any potential relationships between the variables and response as well as the variables with themselves. Since there are 60 variables, eight are randomly selected to be shown below.

```{r}
# univariate plots, for select predictors.  Can we see any patterns?
par(mfrow=c(2,2))
plot(sonar$Y ~ sonar$V10, xlab = "Predictor 10", ylab = "Rock/Mine", col = mycol) #potential relationship
plot(sonar$Y ~ sonar$V25, xlab = "Predictor 25", ylab = "Rock/Mine", col = mycol)
plot(sonar$Y ~ sonar$V39, xlab = "Predictor 39", ylab = "Rock/Mine", col = mycol)
plot(sonar$Y ~ sonar$V58, xlab = "Predictor 58", ylab = "Rock/Mine", col = mycol) #other plots, not so obvious

par(mfrow=c(2,2))
plot(sonar$Y ~ sonar$V7, xlab = "Predictor 7", ylab = "Rock/Mine", col = mycol)   # nothing obvious
plot(sonar$Y ~ sonar$V32, xlab = "Predictor 32", ylab = "Rock/Mine", col = mycol) # low values -> rocks
plot(sonar$Y ~ sonar$V49, xlab = "Predictor 49", ylab = "Rock/Mine", col = mycol) # nothing obvious
plot(sonar$Y ~ sonar$V53, xlab = "Predictor 53", ylab = "Rock/Mine", col = mycol) # nothing obvious 
par(mfrow = c(1,1))

# plot predictors against each other
plot(sonar[,1:10])  # it appears predictors next to each other are pretty similar
plot(sonar[,11:20]) # this trend continues to most of the variables
plot(sonar[,21:30]) 
plot(sonar[,31:40])
plot(sonar[,41:50])
plot(sonar[,51:60]) # less similarity between nearby predictors
```

## Modeling

## Logistic Regression 
When considering a binary outcome, logistic regression is a natural option.  First we check for colinearity.

```{r}
vifmod <- glm(Y~., data = sonar, family = "binomial")
vif(vifmod)
# indicative of lots of collinearity.  
# Since the variables don't have much context, remove largest VIF variable first
# removing variables till all VIF's are under 10

which.max(vif(vifmod))
vifmod <- update(vifmod, .~.- V17)
vif(vifmod)

#
#  Multiple steps removed for space
#


which.max(vif(vifmod))
vifmod <- update(vifmod, .~.- V48)
vif(vifmod)

# create new dataset with non-collinear predictors so it's easier to build the model
newsonar <- sonar[,c(1:8, 10, 12:14, 16, 19, 21, 22, 24, 26, 27, 29, 30, 32, 34, 36, 38, 29, 41, 43, 44, 
                     44, 47, 49:61)]
dim(newsonar)

```

Many variables had to be removed to avoid colinearity.  Although there are two variables that have a VIF over 10 (but less than 11), they were kept in.  I am cautious of removing so many predictors in one step.  A new data set is created after removing the correlated variables.

Before we build the model, we want to split the dataset into a training set and a test set.  We use a 75% training, 25% test split.

```{r}
n <- length(sonar$Y) # number of observations

tr <- sample(1:n, size = floor(n*.75))  # note these random selection of observations will also be
t <- (1:n)[-tr]                         # used when building the random forest 

train <- data.frame(newsonar[tr, ])     
test <- data.frame(newsonar[t, ]) 

trainn <- length(tr)
testn <- length(t)
```

Now we can build the logistic model.  We will take two approaches, using AIC stepwise selection and BIC stepwise selection.  BIC will penalize for more variables.

```{r warning=FALSE}
# AIC stepwise selection
fullmod <- glm(Y ~ ., data = train, family = "binomial")

AICmod <- step(fullmod, direction = "both", trace = FALSE)
summary(AICmod)

# Best model has V1, V4, V10, V12, V13, V14, V22, V26, V30, V36, V38, V41, V44, V47, V49
#                V50, V52, V54, V55, V57, V59, V60 as predictors

# BIC stepwise selection
BICmod <- step(fullmod, direction = "both", trace = FALSE, k = log(trainn))
summary(BICmod)

# Best model has V1, V12, V14, V22, V30, V36, V38, V49, V50, V52, V59, V60
```

As expected, the model chosen using AIC selection has more predictors than the  model chosen using BIC.  To determine which model to use, we will use cross validation to find the model with the lowest error (root mean squared error, RMSE).

```{r warning=FALSE}
AICcv <- cv.glm(train, AICmod, K=5)
(AICcv.e <- AICcv$delta[1])

BICcv <- cv.glm(train, BICmod, K=5)
(BICcv.e <- BICcv$delta[1])

# Appears BIC model has lower error, let's try K = 10, and LOO-CV just to be sure

AICcv <- cv.glm(train, AICmod, K=10)
(AICcv.e <- AICcv$delta[1])

BICcv <- cv.glm(train, BICmod, K=10)
(BICcv.e <- BICcv$delta[1])

# BIC still lower, try LOO CV 

AICcv <- cv.glm(train, AICmod, K=trainn)
(AICcv.e <- AICcv$delta[1])

BICcv <- cv.glm(train, BICmod, K=trainn)
(BICcv.e <- BICcv$delta[1])

# BIC better all around

finalmod <- BICmod
```

After using k = 5, k= 10, and leave-one-out cross validation, the BIC model has the lowest error.  The model chosen using BIC will be our final logistic regression model.

## Random Forest
Since the Sonar dataset is highly colinear, we consider a random forest instead.  We will use the same training set and test set, but with all variables, unlike the logistic model which had many variables removed due to colinearity.

```{r}
ftrain <- data.frame(sonar[tr, ])
ftest <- data.frame(sonar[t, ])

# split into test/train sets with same indecies, but with all 60 predictors

rf1 <- randomForest(Y~., data = ftrain) #default 500 trees, not bad, how will increasing trees improve results?
rf1

rf2 <- randomForest(Y~., data = ftrain, ntree = 1000) # still fast calculation, try more trees
rf2

rf3 <- randomForest(Y~., data = ftrain, ntree = 2000) # better than 1000, try more trees
rf3

# random attempt for the ROC graph to work: builds rf on training set then runs it on test set in same function
rf3.5 <- randomForest(x = ftrain[,1:60], y = ftrain[,61], xtest = ftest[,1:60], ytest = ftest[,61], ntree = 2000)
please <- rf3.5$test

rf4 <- randomForest(Y~., data = ftrain, ntree = 3000) # not substantially better prediction for more trees
rf4

```

After building a model based on 1000, 2000, and 3000 trees, we chose the model with 2000 trees.

## Comparison

Now we want to test our models using the test dataset to see which performs better with lower error.  For the logistic model, we use a threshold of 0.5 to determine whether an object is a rock or a mine.

```{r}
logte_pred <- predict.glm(finalmod, newdata = test, type = "response")
log_te <- data.frame(predictor = logte_pred, known.truth = test$Y)
log_te$predicted <- ifelse(log_te$predictor < 0.5,"M","R")
table(log_te$predicted, log_te$known.truth)

rfte_pred <- predict(rf3, newdata = ftest, type = "response", norm.votes = TRUE, predict.all = FALSE, proximity = FALSE, nodes = FALSE)
rf_te <- data.frame(predicted = rfte_pred, known.truth = test$Y)
table(rf_te$predicted, rf_te$known.truth)
```

On the test data, the logistic model has 20 misclassifications while the random forest model only has 10 misclassifications.  We can check the performance of each model using further calculations.

```{r}
### True Positive Rate (predicted mine, truly mine / true mines)
log_tpr <- round(sum(log_te$predicted == "M" & log_te$known.truth == "M")/sum(log_te$known.truth == "M"), 3)
rf_tpr <- round(sum(rf_te$predicted == "M" & rf_te$known.truth == "M")/sum(rf_te$known.truth == "M"), 3)

### False Postive Rate (predicted mine, truly rock / true rocks)
log_fpr <- round(sum(log_te$predicted == "R" & log_te$known.truth == "M")/sum(log_te$known.truth == "R"), 3)
rf_fpr <- round(sum(rf_te$predicted == "R" & rf_te$known.truth == "M")/sum(rf_te$known.truth == "R"), 3)

### False Negative Rate  (predicted rock, truly mine / true mines)
log_fnr <- round(sum(log_te$predicted == "M" & log_te$known.truth == "R")/sum(log_te$known.truth == "M"), 3)
rf_fnr <- round(sum(rf_te$predicted == "M" & rf_te$known.truth == "R")/sum(rf_te$known.truth == "M"), 3)
  
  
### Accuracy (correct predictions / all predictions)
log_ac <- round((sum(log_te$predicted == "M" & log_te$known.truth == "M") + 
             sum(log_te$predicted == "R" & log_te$known.truth == "R"))/length(log_te[,2]), 3)
rf_ac <- round((sum(rf_te$predicted == "M" & rf_te$known.truth == "M") + 
             sum(rf_te$predicted == "R" & rf_te$known.truth == "R"))/length(rf_te[,2]), 3)

### Precision (number of mines correctly identified to number of mines identified (correct & incorrect))
log_pr <- round(sum(log_te$predicted == "M" & log_te$known.truth == "M")/sum(log_te$predicted == "M"), 3)
rf_pr <- round(sum(rf_te$predicted == "M" & rf_te$known.truth == "M")/sum(rf_te$predicted == "M"), 3)


### Results in Table
lg <- cbind("logistic regression", log_tpr, log_fpr, log_fnr, log_pr, log_ac)
rff <- cbind("random forest", rf_tpr, rf_fpr, rf_fnr, rf_pr, rf_ac)
results <- data.frame(rbind(lg, rff))
colnames(results) <- c("method", "TPR", "FPR", "FNR", "Precision", "Accuracy")
results
```

In every measure, True Positive Rate, False Positive Rate, False Negative Rate, and Accuracy, the random forest model substantially outperforms the logistic regression model.  The success of each model can also be displayed using an ROC curve.

```{r}
threshold=0.5

df <- data.frame(predictor = logte_pred,known.truth = test$Y)
df$predicted.RM<-ifelse(df$predictor < threshold,0,1)
df$known.truth <- ifelse(df$known.truth=="M", 0, 1)

roc.plot<-ggplot(df, aes(d = known.truth, m = predictor)) + 
  geom_roc(n.cuts = 0) + geom_abline(intercept = 0, slope = 1, linetype="dashed") 

roc.plot + annotate("text", x = .75, y = .25, label = paste("AUC =", round(calc_auc(roc.plot)$AUC, 2)))+
  labs(title = "Logistic Regression ROC Curve", x = "False Postive Fraction", y = "True Positive Fraction")

df2 <- data.frame(predictor = please$votes[,2], known.truth = ftest$Y, predicted.RM <- rfte_pred)
df2$predicted.RM<-ifelse(df2$predictor<threshold,0,1)
df2$known.truth <- ifelse(df2$known.truth=="M", 0, 1)

roc.plot2<-ggplot(df2, aes(d = known.truth, m = predictor)) + 
  geom_roc(n.cuts = 0) + geom_abline(intercept = 0, slope = 1, linetype="dashed") 

roc.plot2 + annotate("text", x = .75, y = .25, label = paste("AUC =", round(calc_auc(roc.plot2)$AUC, 2)))+
  labs(title = "Random Forest ROC Curve", x = "False Postive Fraction", y = "True Positive Fraction")
```

A perfect model would result in an AUC=1, thus again, the random forest better predicts rocks or mines.

## Results, Operations

### References

Dua, D., & Graff, C. (2019). Machine Learning Repository. Retrieved from University of Caliofrnia, School of Information and Computer Science: http://archive.ics.uci.edu/ml

Kirasich, K., Smith, T., & Sadler, B. (2018). Random Forest vs Logistic Regression: Binary Classification for Heterogeneous Datasets. SMU Data Science Review. Retrieved from https://scholar.smu.edu/cgi/viewcontent.cgi?article=1041&context=datasciencereview

Oshiro, T. M., Perez, P. S., & Baranauskas, J. A. (n.d.). How Many Trees in a Random Forest? Machine Learning and Data Mining in Pattern Recognition, 154-168.

